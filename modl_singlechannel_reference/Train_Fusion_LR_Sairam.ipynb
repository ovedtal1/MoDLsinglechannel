{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'Python 3.10.12'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. ENOSPC: no space left on device, write"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import os, sys\n",
    "import logging\n",
    "import random\n",
    "import h5py\n",
    "import shutil\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import sigpy.plot as pl\n",
    "import torch\n",
    "import sigpy as sp\n",
    "import torchvision\n",
    "from torch import optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib\n",
    "# import custom libraries\n",
    "from utils import transforms as T\n",
    "from utils import subsample as ss\n",
    "from utils import complex_utils as cplx\n",
    "from utils.resnet2p1d import generate_model\n",
    "from utils.flare_utils import roll\n",
    "# import custom classes\n",
    "from utils.datasets_Sairam import SliceData\n",
    "from subsample_fastmri import MaskFunc\n",
    "from MoDL_single import UnrolledModel\n",
    "import argparse\n",
    "from models.SAmodel import MyNetwork\n",
    "from models.Unrolled import Unrolled\n",
    "from models.UnrolledRef import UnrolledRef\n",
    "from models.UnrolledTransformer import UnrolledTrans\n",
    "import matplotlib.pyplot as plt\n",
    "from ImageFusionBlock import ImageFusionBlock\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "%load_ext autoreload\n",
    "%autoreload 0\n",
    "from ImageFusion_Dualbranch_Fusion.densefuse_net import DenseFuseNet\n",
    "from ImageFusion_Dualbranch_Fusion.channel_fusion import channel_f as channel_fusion\n",
    "import itertools\n",
    "from RCAN import CombinedNetwork\n",
    "from models.FusionNet import FusionNet\n",
    "from recon_net_wrap import ViTfuser\n",
    "#from UnrolledViT import UnrolledViT\n",
    "from UnrolledViT import UnrolledViT\n",
    "\n",
    "from fastmri.data import transforms, subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransform:\n",
    "    \"\"\"\n",
    "    Data Transformer for training unrolled reconstruction models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mask_func, args, use_seed=False):\n",
    "        self.mask_func = mask_func\n",
    "        self.use_seed = use_seed\n",
    "        self.rng = np.random.RandomState()\n",
    "    def get_mask_func(self, factor):\n",
    "        center_fractions = 0.1 * 4/factor# RandomMaskFuncEquiSpacedMaskFunc\n",
    "        mask_func = subsample.EquiSpacedMaskFunc(\n",
    "        center_fractions=[center_fractions],\n",
    "        accelerations=[factor], \n",
    "        )\n",
    "        return mask_func\n",
    "    \n",
    "    def __call__(self, kspace, target, reference_kspace, reference,slice):\n",
    "        im_lowres = abs(sp.ifft(sp.resize(sp.resize(kspace,(256,24)),(256,160))))\n",
    "        magnitude_vals = im_lowres.reshape(-1)\n",
    "        k = int(round(0.05 * magnitude_vals.shape[0]))\n",
    "        scale = magnitude_vals[magnitude_vals.argsort()[::-1][k]]\n",
    "        kspace = kspace/scale\n",
    "        target = target/scale\n",
    "        # Convert everything from numpy arrays to tensors\n",
    "        kspace_torch = cplx.to_tensor(kspace).float()   \n",
    "        target_torch = cplx.to_tensor(target).float()  \n",
    "        target_torch = T.ifft2(T.fft2(target_torch)) \n",
    "        # Use poisson mask instead\n",
    "        #mask2 = sp.mri.poisson((256,160), 5, calib=(18, 14), dtype=float, crop_corner=False, return_density=True, seed=0, max_attempts=6, tol=0.01)\n",
    "        #mask2[128-10:128+9,80-8:80+7] = 1\n",
    "        #mask_torch = torch.stack([torch.tensor(mask2).float(),torch.tensor(mask2).float()],dim=2)\n",
    "        #mask_torch = T.kspace_crop(mask_torch,0.67)\n",
    "        #kspace_torch = T.kspace_cut(mask_torch,0.5)\n",
    "        kspace_torch = T.awgn_torch(kspace_torch,30,L=1) # 10dB for simulations\n",
    "        ## Masking\n",
    "        mask_func = self.get_mask_func(3)\n",
    "        kspace_torch = transforms.apply_mask(kspace_torch, mask_func)[0]\n",
    "        # kspace_torch = kspace_torch*mask_torch # For poisson\n",
    "        \n",
    "        mask = np.abs(cplx.to_numpy(kspace_torch))!=0\n",
    "        mask_torch = torch.stack([torch.tensor(mask).float(),torch.tensor(mask).float()],dim=2)\n",
    "        \n",
    "        ### Reference addition ###\n",
    "        im_lowres_ref = abs(sp.ifft(sp.resize(sp.resize(reference_kspace,(172,24)),(172,108))))\n",
    "        magnitude_vals_ref = im_lowres_ref.reshape(-1)\n",
    "        k_ref = int(round(0.05 * magnitude_vals_ref.shape[0]))\n",
    "        scale_ref = magnitude_vals_ref[magnitude_vals_ref.argsort()[::-1][k_ref]]\n",
    "        reference = reference / scale_ref\n",
    "        reference_torch = cplx.to_tensor(reference).float()\n",
    "        reference_torch_kspace = T.fft2(reference_torch)\n",
    "        reference_torch_kspace = reference_torch_kspace\n",
    "        reference_torch = T.ifft2(reference_torch_kspace)\n",
    "        \n",
    "\n",
    "        return kspace_torch,target_torch,mask_torch, reference_torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(args):\n",
    "    # Generate k-t undersampling masks\n",
    "    train_mask = MaskFunc([0.08],[4])\n",
    "    train_data = SliceData(\n",
    "        root=str(args.data_path),\n",
    "        transform=DataTransform(train_mask, args),\n",
    "        sample_rate=1\n",
    "    )\n",
    "    return train_data\n",
    "def create_data_loaders(args):\n",
    "    train_data = create_datasets(args)\n",
    "#     print(train_data[0])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return train_loader\n",
    "def build_optim(args, params):\n",
    "    optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay=args.weight_decay)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "params = Namespace()\n",
    "#params.data_path = \"./registered_data/patient23b/\"\n",
    "params.data_path = \"./reg_data_sairam/\"\n",
    "params.batch_size = 2 #4\n",
    "params.num_grad_steps = 1 #4\n",
    "params.num_cg_steps = 8 #8\n",
    "params.share_weights = True\n",
    "params.modl_lamda = 0.05\n",
    "params.lr = 0.0005 #0.0005\n",
    "#params.lr = 0.0001\n",
    "params.weight_decay = 0\n",
    "params.lr_step_size = 4\n",
    "params.lr_gamma = 0.3\n",
    "params.epoch = 41\n",
    "params.reference_mode = 1\n",
    "params.reference_lambda = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = create_data_loaders(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "#model_ft = models.resnet18(weights='DEFAULT').to(device).requires_grad_(False)\n",
    "#model_ft.fc = nn.Identity()\n",
    "#model_ft = models.vgg16(weights='DEFAULT').to(device)#.requires_grad_(False)\n",
    "from FSloss_wrap import VGGLoss,ResNet18Backbone,FeatureEmbedding,contrastive_loss,VGGPerceptualLoss\n",
    "#VGGloss = VGGLoss().to(device)\n",
    "VGGloss = VGGPerceptualLoss().to(device)\n",
    "#UFLoss = ResNet18Backbone().to(device)\n",
    "#UFLoss = VGGLoss().to(device)\n",
    "#UFLoss = models.vgg16(pretrained=True).features[:8+1].to(device)\n",
    "#UFLoss.eval()\n",
    "\n",
    "def extract_patches(images, patch_size=(20, 20), stride=(20, 20)):\n",
    "    # images: Tensor of shape (batch_size, 1, 180, 110)\n",
    "    patches = images.unfold(2, patch_size[0], stride[0]).unfold(3, patch_size[1], stride[1])\n",
    "    patches = patches.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "    patches = patches.view(images.size(0), -1, 1, patch_size[0], patch_size[1])\n",
    "    return patches  # Returns patches of shape (batch_size, num_patches, 1, patch_size[0], patch_size[1])\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "patch_size = (20, 20)\n",
    "stride = (20, 20)  # Non-overlapping patches\n",
    "def feature_space_loss(features1, features2):\n",
    "    return F.mse_loss(features1, features2)\n",
    "def pad_image(images):\n",
    "    # images: Tensor of shape (batch_size, 1, 172, 108)\n",
    "    padded_images = F.pad(images, (6, 6, 4, 4), mode='constant', value=0)\n",
    "    return padded_images  # Shape will be (batch_size, 1, 180, 120)\n",
    "#modelLoss = ResNet18Backbone().to(device)\n",
    "#embedding_model = FeatureEmbedding(modelLoss).to(device)\n",
    "#memory_bank = torch.randn(16, 128)  # Assuming num_patches is the number of different patches stored.\n",
    "#memory_bank = nn.functional.normalize(memory_bank, p=2, dim=1)  # Normalize the memory bank vectors\n",
    "\n",
    "\n",
    "from vision_transformer import VisionTransformer\n",
    "net = VisionTransformer(\n",
    "  avrg_img_size=320,\n",
    "  patch_size = (10,10),\n",
    "  in_chans=1,\n",
    "  embed_dim=64,\n",
    "  depth=10,\n",
    "  num_heads=16\n",
    "\n",
    ")\n",
    "\n",
    "from recon_net import ReconNet\n",
    "model = UnrolledViT(params).to(device)\n",
    "#model2 = ReconNet(net).to(device)#.requires_grad_(False)\n",
    "#cp = torch.load('./lsdir-2x+hq50k_vit_epoch_60.pt', map_location=device)\n",
    "#model2.load_state_dict(cp['model_state_dict'])\n",
    "\n",
    "\"\"\"\n",
    "model.requires_grad_(False)\n",
    "\n",
    "for net in model.similaritynets:\n",
    "    net.param1.requires_grad_(True)\n",
    "    net.param2.requires_grad_(True)\n",
    "    #net.recon_net.net.head.requires_grad_(True)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer=optimizer, \n",
    "    max_lr=0.0001,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=params.epoch,\n",
    "    pct_start=0.01,\n",
    "    anneal_strategy='linear',\n",
    "    cycle_momentum=False,\n",
    "    base_momentum=0., \n",
    "    max_momentum=0.,\n",
    "    div_factor = 25.,\n",
    "    final_div_factor=1.,\n",
    ")\n",
    "\"\"\"\n",
    "optimizer = build_optim(params,  model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, params.lr_step_size, params.lr_gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load for fine-tunning\n",
    "\"\"\"\n",
    "checkpoint_file = \"./L2_checkpoints_poisson_x2_FusionNet/model_30.pt\"\n",
    "checkpoint = torch.load(checkpoint_file,map_location=device)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.recon_net.requires_grad_(True)\n",
    "\"\"\"\n",
    "from fastmri.losses import SSIMLoss\n",
    "criterion = SSIMLoss().to(device)\n",
    "criterionMSE = nn.MSELoss()\n",
    "#criterion = nn.L1Loss()\n",
    "\n",
    "epochs_plot = []\n",
    "losses_plot = []\n",
    "\n",
    "for epoch in range(params.epoch):\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    running_loss = 0.0\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        input,target,mask,reference = data\n",
    "        input = input.to(device).float()\n",
    "        target = target.to(device).float()\n",
    "        mask = mask.to(device)\n",
    "        reference = reference.to(device).float()\n",
    "        image = T.ifft2(input)\n",
    "        image = image.permute(0,3,1,2)\n",
    "\n",
    "        #print(f'image shape: {image.shape}')\n",
    "        #print(f'reference shape: {reference.shape}')\n",
    "        ## Target\n",
    "        target_image = target.permute(0,3,1,2) \n",
    "        #print(f'ref size: {reference_image.shape}')\n",
    "        real_part_tar = target_image[:,0,:,:].unsqueeze(1)\n",
    "        imag_part_tar = target_image[:,1,:,:].unsqueeze(1)\n",
    "        mag_tar = torch.sqrt(real_part_tar**2 + imag_part_tar**2).to(device)\n",
    "        ## Reference\n",
    "        ref_image = reference.permute(0,3,1,2) \n",
    "        real_part_ref = ref_image[:,0,:,:].unsqueeze(1)\n",
    "        imag_part_ref = ref_image[:,1,:,:].unsqueeze(1)\n",
    "        mag_ref = torch.sqrt(real_part_ref**2 + imag_part_ref**2).to(device)\n",
    "        \"\"\"\n",
    "        in_pad, wpad, hpad = model2.pad(mag_tar)\n",
    "        input_norm,mean,std = model2.norm(in_pad.float())\n",
    "        # Feature extract\n",
    "        #print(mag_tar.shape)\n",
    "        mag_tar = torch.cat((mag_tar,mag_tar,mag_tar),dim =1).to(device)\n",
    "        \n",
    "        features_target = vgg16_model(torch.cat((mag_tar,mag_tar,mag_tar),dim =1).to(device)).data\n",
    "        \"\"\"\n",
    "        #print(f'Features target: {features_target.shape}')\n",
    "        im_out = model(input,reference)#.squeeze(3)\n",
    "\n",
    "        \"\"\"\n",
    "        # Plot the concatenated image\n",
    "        real_part = image[0,0,:,:]\n",
    "        imag_part = image[0,1,:,:]\n",
    "        mag_image = torch.sqrt(real_part**2 + imag_part**2)\n",
    "        real_part_ref = reference[0,:,:,0]\n",
    "        imag_part_ref = reference[0,:,:,1]\n",
    "        mag_ref = torch.sqrt(real_part_ref**2 + imag_part_ref**2)\n",
    "        mag_ref = mag_ref.cpu().detach().numpy()\n",
    "        print(f'Mag ref: {mag_ref.shape}')\n",
    "        import matplotlib.pyplot as plt\n",
    "        %matplotlib inline\n",
    "        print(im_out.shape)\n",
    "        print(mag_tar.shape)\n",
    "        im_out = im_out.cpu().detach().numpy().squeeze(0)\n",
    "        concat = np.concatenate((mag_ref,mag_image.cpu().detach().numpy(),np.abs(im_out),mag_tar.squeeze(0).cpu().detach().numpy()),axis=1)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(concat, cmap='gray')\n",
    "        plt.title('reference                         in                           out                       target   ')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        l = torch(mag_tar)\n",
    "        \"\"\"\n",
    "        #loss = criterion(im_out,features_target)\n",
    "        # SSIM\n",
    "        maxval = torch.max(torch.cat((im_out,mag_tar.permute(0,2,3,1)),dim=1))\n",
    "        im_out = im_out.permute(0,3,1,2)\n",
    "\n",
    "        #features_out = vgg16_model(torch.cat((im_out,im_out,im_out),dim =1))\n",
    "        \n",
    "        #print(features_out.shape)\n",
    "        data_range = torch.tensor([maxval], device=device).view(1, 1, 1, 1).expand(im_out.size(0), im_out.size(1), im_out.size(2)-6, im_out.size(3)-6)\n",
    "        #print(mag_tar.shape)\n",
    "        #print(im_out.shape)\n",
    "        #print(data_range.shape)\n",
    "        # SSIM\n",
    "        #loss = criterion(im_out, mag_tar.to(device), data_range.to(device))\n",
    "        # pad:\n",
    "        im_out_pad = torch.cat((im_out,im_out,im_out),dim =1)/maxval\n",
    "        mag_tar_pad = torch.cat((mag_tar,mag_tar,mag_tar),dim =1)/maxval\n",
    "        #loss = nn.MSELoss()(model_ft.features(im_out_pad), model_ft.features(mag_tar_pad))\n",
    "        \n",
    "        # SSIM + style\n",
    "        #print(f'ssim is : {+ criterion(im_out, mag_tar.to(device), data_range.to(device))}')\n",
    "        loss =  1*criterion(im_out, mag_tar.to(device), data_range.to(device)) + 0.3*VGGloss(im_out,mag_ref.to(device)) + 0.1*criterion(im_out, mag_ref.to(device), data_range.to(device)) # For tests2\n",
    "\n",
    "        # SSIM loss for grant\n",
    "        #loss = criterion(im_out, mag_tar.to(device), data_range.to(device))\n",
    "        #loss = criterionMSE(im_out,mag_tar.to(device))\n",
    "        \"\"\"\n",
    "        padded_out = pad_image(im_out)\n",
    "        padded_target = pad_image(mag_tar)\n",
    "        #print(f'padded out size: {padded_out.shape}')\n",
    "        out_patches = extract_patches(padded_out, patch_size, stride)\n",
    "        target_patches = extract_patches(padded_target, patch_size, stride)\n",
    "        #print(f'out_patches size: {out_patches.shape}')\n",
    "        loss = 0\n",
    "        loss_tmp = 0\n",
    "        # Forward pass for each patch\n",
    "        for i in range(out_patches.size(1)):\n",
    "            image_patch = out_patches[:, i]  # Shape: (batch_size, 1, 20, 20)\n",
    "            target_patch = target_patches[:, i]  # Shape: (batch_size, 1, 20, 20)\n",
    "            \n",
    "            #Tripple to use in resnet:\n",
    "            #image_patch = torch.cat((image_patch,image_patch,image_patch),dim=1)\n",
    "            #target_patch = torch.cat((target_patch,target_patch,target_patch),dim=1)\n",
    "            #print(f'image patch: {image_patch.shape}')\n",
    "            # Compute feature space loss\n",
    "\n",
    "            #features = UFLoss(image_patch,target_patch)\n",
    "            #target_features = UFLoss(target_patch)\n",
    "        \n",
    "            loss_tmp += VGGloss(image_patch,target_patch.to(device))  #divide beacuse of channels\n",
    "        loss = loss_tmp/(170*100) + criterion(im_out, mag_tar.to(device), data_range.to(device))\n",
    "        \"\"\"\n",
    "        # L1\n",
    "        #loss = criterion(features_out, features_out)\n",
    "        # MSE\n",
    "        #loss = criterion(im_out,mag_tar.permute(0,2,3,1))\n",
    "        \n",
    "        running_loss = running_loss + loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
    "        if iter % 400 == 0:\n",
    "            logging.info(\n",
    "                f'Epoch = [{epoch:3d}/{params.epoch:3d}] '\n",
    "                f'Iter = [{iter:4d}/{len(train_loader):4d}] '\n",
    "                f'Loss = {loss.item():.4g} Avg Loss = {avg_loss:.4g}'\n",
    "            )\n",
    "    #Saving the model\n",
    "    exp_dir = \"L2_checkpoints_Sairam_fuser_new/\"\n",
    "    if epoch % 20 == 0:\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'params': params,\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'exp_dir': exp_dir\n",
    "            },\n",
    "            f=os.path.join(exp_dir, 'model_%d.pt'%(epoch))\n",
    "    )\n",
    "    running_loss = running_loss / len(train_loader)\n",
    "    #scheduler.step(running_loss)\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    print(f'Epoch {epoch+1}, Learning rate: {current_lr}')\n",
    "\n",
    "    #print(f'Epoch {epoch+1}, Learning rate: {scheduler.get_last_lr()[0]}')\n",
    "    # Append epoch and average loss to plot lists\n",
    "    epochs_plot.append(epoch)\n",
    "    losses_plot.append(running_loss)\n",
    "\n",
    "# Plotting the loss curve\n",
    "plt.figure()\n",
    "plt.plot(epochs_plot, losses_plot, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('SA unrolled with Reference L2 train Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(exp_dir, 'loss_plot_plato_down.png'))  # Save plot as an image\n",
    "\n",
    "# Save all_losses to a file for later comparison\n",
    "losses_file = os.path.join(exp_dir, 'all_losses.txt')\n",
    "with open(losses_file, 'w') as f:\n",
    "    for loss in losses_plot:\n",
    "        f.write(f'{loss}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
